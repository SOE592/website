---
title: "Intro to time series analysis"
subtitle: "SOE 592 – Intro to Time Series Analysis"
author: "Mark Scheuerell"
date: "3 Jan 2024"
output:
  ioslides_presentation:
    css: lecture_slides.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Topics

### Characteristics of time series (ts)

### Classical decomposition

### Characteristics of time series

* Expectation, mean & variance
* Covariance & correlation
* Stationarity
* Autocovariance & autocorrelation
* Correlograms


## What is a time series?



## {.flexbox .vcenter}

<div class="centered">
![](https://www.esrl.noaa.gov/gmd/webdata/ccgg/trends/co2_data_mlo.png){width=90%}
</div>



## Classification of time series | By some _index set_

Discrete time; $x_t$    

* Equally spaced: $t = \{1,2,3,4,5\}$  
* Equally spaced w/ missing value: $t = \{1,2,4,5,6\}$  
* Unequally spaced: $t = \{2,3,4,6,9\}$  


## Classification of time series | By the _underlying process_    

Discrete (eg, total # of fish caught per trawl)

Continuous (eg, salinity, temperature)


## Classification of time series | By the _number of values recorded_    

Univariate/scalar (eg, total # of fish caught)    

Multivariate/vector (eg, # of each spp of fish caught)


## Classification of time series | By the _type of values recorded_    

Integer (eg, # of fish in 5 min trawl = 2413)

Real (eg, fish mass = 10.2 g)


## Classification of time series | We will focus on integers & real-values in discrete time

Univariate $(x_t)$

<br>

Multivariate $\begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix}_t$


# Analysis of time series


## Statistical analyses of time series

Most statistical analyses are concerned with estimating properties of a population from a sample

For example, we use fish caught in a seine to infer the mean size of fish in a lake


## Statistical analyses of time series

Time series analysis, however, presents a different situation:

* Although we could vary the _length_ of an observed time series, it is often impossible to make multiple observations at a _given_ point in time


## Statistical analyses of time series

Time series analysis, however, presents a different situation:

* Although we could vary the _length_ of an observed time series, it is often impossible to make multiple observations at a _given_ point in time

For example, one can’t observe today’s closing price of Microsoft stock more than once  

Thus, conventional statistical procedures, based on large sample estimates, are inappropriate


## Descriptions of time series

```{r ex_ts_plot_www, fig.cap = "Number of users connected to the internet"}
data(WWWusage)
par(mai = c(0.9,0.9,0.1,0.1), omi = c(0,0,0,0))
plot.ts(WWWusage, ylab = "", las = 1, col = "blue", lwd = 2)
```


## Descriptions of time series

```{r ex_ts_plot_lynx, fig.cap = "Number of lynx trapped in Canada from 1821-1934"}
data(lynx)
par(mai = c(0.9,0.9,0.1,0.1), omi = c(0,0,0,0))
plot.ts(lynx, ylab = "", las = 1, col = "blue", lwd = 2)
```


## Classical decomposition

### Model time series $\{x_t\}$ as a combination of

1. trend ($m_t$)  
2. seasonal component ($s_t$)  
3. remainder ($e_t$)

$x_t = m_t + s_t + e_t$


## Classical decomposition | 1. The trend ($m_t$)

We need a way to extract the so-called _signal_ from the _noise_

One common method is via "linear filters"

Linear filters can be thought of as "smoothing" the data


## Classical decomposition | 1. The trend ($m_t$)

Linear filters typically take the form

$$
\hat{m}_t = \sum_{i=-\infty}^{\infty} \lambda_i x_{t+1}
$$


## Classical decomposition | 1. The trend ($m_t$)

For example, a moving average

$$
\hat{m}_t = \sum_{i=-a}^{a} \frac{1}{2a + 1} x_{t+i}
$$


## Classical decomposition | 1. The trend ($m_t$)

For example, a moving average

$$
\hat{m}_t = \sum_{i=-a}^{a} \frac{1}{2a + 1} x_{t+i}
$$

If $a = 1$, then

$$
\hat{m}_t = \frac{1}{3}(x_{t-1} + x_t + x_{t+1})
$$


## Classical decomposition | 1. The trend ($m_t$)

For example, a moving average

$$
\hat{m}_t = \sum_{i=-a}^{a} \frac{1}{2a + 1} x_{t+i}
$$

As $a$ increases, the estimated trend becomes more smooth


## Example of linear filtering

```{r plot_airpass, fig.cap = "Monthly airline passengers from 1949-1960"}
xx <- AirPassengers
par(mai = c(0.9,0.9,0.1,0.1), omi = c(0,0,0,0))
plot.ts(xx, las = 1, ylab = "")
```


## Example of linear filtering

```{r plot_airpass_fltr1, fig.cap = "Monthly airline passengers from 1949-1960"}
par(mai = c(0.9,0.9,0.1,0.1), omi = c(0,0,0,0))
plot.ts(xx, las = 1, ylab = "")
## weights for moving avg
fltr <- c(1,1,1)/3
trend <- filter(xx, filter=fltr, method="convo", sides=2)
lines(trend, col = "blue", lwd = 2)
text(x = 1949, y = max(trend, na.rm = TRUE),
     labels = expression(paste("a = 1; ", lambda, " = 1/3")),
     adj = c(0,0), col = "blue")
```


## Example of linear filtering

```{r plot_airpass_fltr2, fig.cap = "Monthly airline passengers from 1949-1960"}
par(mai = c(0.9,0.9,0.1,0.1), omi = c(0,0,0,0))
plot.ts(xx, las = 1, ylab = "")
## weights for moving avg
fltr2 <- rep(1,9)/9
trend2 <- filter(xx, filter=fltr2, method="convo", sides=2)
lines(trend, col = "blue", lwd = 2)
lines(trend2, col = "darkorange", lwd = 2)
text(x = 1949, y = max(trend, na.rm = TRUE),
     labels = expression(paste("a = 1; ", lambda, " = 1/3")),
     adj = c(0,0), col = "blue")
text(x = 1949, y = max(trend, na.rm = TRUE)*0.9,
     labels = expression(paste("a = 4; ", lambda, " = 1/9")),
     adj = c(0,0), col = "darkorange")
```


## Example of linear filtering

```{r plot_airpass_fltr3, fig.cap = "Monthly airline passengers from 1949-1960"}
par(mai = c(0.9,0.9,0.1,0.1), omi = c(0,0,0,0))
plot.ts(xx, las = 1, ylab = "")
## weights for moving avg
fltr3 <- rep(1,27)/27
trend3 <- filter(xx, filter=fltr3, method="convo", sides=2)
lines(trend, col = "blue", lwd = 2)
lines(trend2, col = "darkorange", lwd = 2)
lines(trend3, col = "darkred", lwd = 2)
text(x = 1949, y = max(trend, na.rm = TRUE),
     labels = expression(paste("a = 1; ", lambda, " = 1/3")),
     adj = c(0,0), col = "blue")
text(x = 1949, y = max(trend, na.rm = TRUE)*0.9,
     labels = expression(paste("a = 4; ", lambda, " = 1/9")),
     adj = c(0,0), col = "darkorange")
text(x = 1949, y = max(trend, na.rm = TRUE)*0.8,
     labels = expression(paste("a = 13; ", lambda, " = 1/27")),
     adj = c(0,0), col = "darkred")
```


## Classical decomposition | 2. Seasonal effect ($s_t$)

Once we have an estimate of the trend $\hat{m}_t$, we can estimate $\hat{s}_t$ simply by subtraction:

$$
\hat{s}_t = x_t - \hat{m}_t
$$


## Classical decomposition

```{r plot_airpass_decomp_seas, fig.cap = ""}
seas <- trend2 - xx
  
par(mai = c(0.9,0.9,0.1,0.1), omi = c(0,0,0,0))
plot.ts(seas, las = 1, ylab = "")
# text(x = 1949, y = max(trend, na.rm = TRUE)*0.9,
#      labels = expression(paste(lambda, " = 1/9")),
#      adj = c(0,0), col = "darkorange")
```

Seasonal effect ($\hat{s}_t$), assuming $\lambda = 1/9$


## Classical decomposition | 2. Seasonal effect ($s_t$)

But, $\hat{s}_t$ really includes the remainder $e_t$ as well

$$
\begin{align}
\hat{s}_t &= x_t - \hat{m}_t \\
(s_t + e_t) &= x_t - m_t
\end{align}
$$

## Classical decomposition | 2. Seasonal effect ($s_t$)

So we need to estimate the _mean_ seasonal effect as 

$$
\hat{s}_{Jan} = \sum \frac{1}{(N/12)} \{s_1, s_{13}, s_{25}, \dots \} \\
\hat{s}_{Feb} = \sum \frac{1}{(N/12)} \{s_2, s_{14}, s_{26}, \dots \} \\
\vdots \\
\hat{s}_{Dec} = \sum \frac{1}{(N/12)} \{s_{12}, s_{24}, s_{36}, \dots \} \\
$$


## Mean seasonal effect ($s_t$)

```{r mean_seasonal_effects}
seas_2 <- decompose(xx)$seasonal
par(mai = c(0.9,0.9,0.1,0.1), omi = c(0,0,0,0))
plot.ts(seas_2, las = 1, ylab = "")
```


## Classical decomposition | 3. Remainder ($e_t$)

Now we can estimate $e_t$ via subtraction:

$$
\hat{e}_t = x_t - \hat{m}_t - \hat{s}_t
$$


## Remainder ($e_t$)

```{r errors}
ee <- decompose(xx)$random
par(mai = c(0.9,0.9,0.1,0.1), omi = c(0,0,0,0))
plot.ts(ee, las = 1, ylab = "")
```


## Let's try a different model | With some other assumptions

1. Log-transform data

2. Linear trend


## Log-transformed data

```{r plot_ln_airpass, fig.cap = "Monthly airline passengers from 1949-1960"}
lx <- log(AirPassengers)
par(mai = c(0.9,0.9,0.1,0.1), omi = c(0,0,0,0))
plot.ts(lx, las = 1, ylab = "")
```


## The trend ($m_t$)

```{r plot_lin_trend}
tt <- as.vector(time(xx))
cc <- coef(lm(lx ~ tt))
pp <- cc[1] + cc[2] * tt
  
par(mai = c(0.9,0.9,0.1,0.1), omi = c(0,0,0,0))
plot(tt, lx, type="l", las = 1,
     xlab = "Time", ylab = "")
lines(tt, pp, col = "blue", lwd = 2)
```


## Seasonal effect ($s_t$) with error ($e_t$)

```{r seas_ln_dat}
par(mai = c(0.9,0.9,0.1,0.1), omi = c(0,0,0,0))
plot.ts(lx-pp)
```


## Mean seasonal effect ($s_t$)

```{r mean_seas_effects}
## length of ts
ll <- length(lx)
## frequency (ie, 12)
ff <- frequency(lx)
## number of periods (years); %/% is integer division
periods <- ll %/% ff
## index of cumulative month
index <- seq(1,ll,by=ff) - 1
## get mean by month
mm <- numeric(ff)
for(i in 1:ff) {
  mm[i] <- mean(lx[index+i], na.rm=TRUE)
}
## subtract mean to make overall mean=0
mm <- mm - mean(mm)
seas_2 <- ts(rep(mm, periods+1)[seq(ll)],
               start=start(lx), 
               frequency=ff)
par(mai = c(0.9,0.9,0.1,0.1), omi = c(0,0,0,0))
plot.ts(seas_2, las = 1, ylab = "")
```


## Remainder ($e_t$)

```{r ln_errors}
le <- lx - pp - seas_2
par(mai = c(0.9,0.9,0.1,0.1), omi = c(0,0,0,0))
plot.ts(le, las = 1, ylab = "")
```


## Expectation & the mean

The expectation ($E$) of a variable is its mean value in the population

$\text{E}(x) \equiv$ mean of $x = \mu$

We can estimate $\mu$ from a sample as

$$
m = \frac{1}{N} \sum_{i=1}^N{x_i}
$$


## Variance

$\text{E}([x - \mu]^2) \equiv$ expected deviations of $x$ about $\mu$

$\text{E}([x - \mu]^2) \equiv$ variance of $x = \sigma^2$

We can estimate $\sigma^2$ from a sample as

$$
s^2 = \frac{1}{N-1}\sum_{i=1}^N{(x_i - m)^2}
$$


## Covariance

If we have two variables, $x$ and $y$, we can generalize variance

$$
\sigma^2 = \text{E}([x_i - \mu][x_i - \mu])
$$

into _covariance_
 
$$
\gamma_{x,y} = \text{E}([x_i - \mu_x][y_i - \mu_y])
$$


## Covariance

If we have two variables, $x$ and $y$, we can generalize variance

$$
\sigma^2 = \text{E}([x_i - \mu][x_i - \mu])
$$

into _covariance_
 
$$
\gamma_{x,y} = \text{E}([x_i - \mu_x][y_i - \mu_y])
$$

We can estimate $\gamma_{x,y}$ from a sample as

$$
\text{Cov}(x,y) = \frac{1}{N-1}\sum_{i=1}^N{(x_i - m_x)(y_i - m_y)}
$$


## Graphical example of covariance

```{r, fig.align="center"}
# create dummy x set
xx <- runif(25, 0, 10)
yy <- 1 + 0.3*xx + rnorm(25,0,1.5)

par(mai=c(1,1,0,0), omi=c(0,0,0.5,1))
plot(xx, yy, pch=16, asp=1, las = 1,
     cex=1.2, cex.lab=1.2, col="black",
     xlab=expression(italic(x)),
     ylab=expression(italic(y)),
     main="")

```


## Graphical example of covariance

```{r, fig.align="center"}
par(mai=c(1,1,0,0), omi=c(0,0,0.5,1))
plot(xx, yy, pch=16, asp=1, las = 1,
     cex=1.2, cex.lab=1.2, col="black",
     xlab=expression(italic(x)),
     ylab=expression(italic(y)),
     main="")

# add mean lines	
abline(h=mean(yy), lty="dashed")
abline(v=mean(xx), lty="dashed")
# add labels for means
mtext(side=3, line=0.4, at=mean(xx), expression(italic(m[x])))
mtext(side=4, line=0.5, at=mean(yy), expression(italic(m[y])), las=1)

# add quadrant labels
mtext(side=3, line=0.4, at=(mean(xx)+par()$usr[1])/2, expression((italic(x[i])-italic(m[x])) < 0))
mtext(side=3, line=0.4, at=(mean(xx)+par()$usr[2])/2, expression((italic(x[i])-italic(m[x])) > 0))
mtext(side=4, line=0.5, at=(mean(yy)+par()$usr[3])/2, expression((italic(y[i])-italic(m[y])) < 0), las=1)
mtext(side=4, line=0.5, at=(mean(yy)+par()$usr[4])/2, expression((italic(y[i])-italic(m[y])) > 0), las=1)
```


## Graphical example of covariance

```{r, fig.align="center"}
par(mai=c(1,1,0,0), omi=c(0,0,0.5,1))
plot(xx, yy, pch=16, asp=1, las = 1,
     cex=1.2, cex.lab=1.2, col="gray",
     xlab=expression(italic(x)),
     ylab=expression(italic(y)),
     main="")

# add mean lines	
abline(h=mean(yy), lty="dashed")
abline(v=mean(xx), lty="dashed")
# add labels for means
mtext(side=3, line=0.4, at=mean(xx), expression(italic(m[x])))
mtext(side=4, line=0.5, at=mean(yy), expression(italic(m[y])), las=1)

# add quadrant labels
mtext(side=3, line=0.4, at=(mean(xx)+par()$usr[1])/2, expression((italic(x[i])-italic(m[x])) < 0))
mtext(side=3, line=0.4, at=(mean(xx)+par()$usr[2])/2, expression((italic(x[i])-italic(m[x])) > 0))
mtext(side=4, line=0.5, at=(mean(yy)+par()$usr[3])/2, expression((italic(y[i])-italic(m[y])) < 0), las=1)
mtext(side=4, line=0.5, at=(mean(yy)+par()$usr[4])/2, expression((italic(y[i])-italic(m[y])) > 0), las=1)

# get indices for data pairs with neg cov
negC <- (xx<mean(xx) & yy>mean(yy)) | (xx>mean(xx) & yy<mean(yy))

# overlay pos & neg cov values
points(xx[negC], yy[negC], pch="-", cex=2, col="darkred")
points(xx[!negC], yy[!negC], pch="+", cex=1.5, col="blue")
```


## Correlation

_Correlation_ is a dimensionless measure of the linear association between 2 variables, $x$ & $y$

It is simply the covariance standardized by the standard deviations

$$
\rho_{x,y} = \frac{\gamma_{x,y}}{\sigma_x \sigma_y}
$$

$$
-1 < \rho_{x,y} < 1
$$


## Correlation

_Correlation_ is a dimensionless measure of the linear association between 2 variables $x$ & $y$

It is simply the covariance standardized by the standard deviations

$$
\rho_{x,y} = \frac{\gamma_{x,y}}{\sigma_x \sigma_y}
$$

We can estimate $\rho_{x,y}$ from a sample as

$$
\text{Cor}(x,y) = \frac{\text{Cov}(x,y)}{s_x s_y}
$$


## Stationarity & the mean

Consider a single value, $x_t$


## Stationarity & the mean

Consider a single value, $x_t$

$\text{E}(x_t)$ is taken across an ensemble of _all_ possible time series


## Stationarity & the mean

```{r station_in_mean}
nn <- 200
tt <- 40
ww <- matrix(rnorm(nn*tt), tt, nn)

mm <- apply(ww, 1, mean)

par(mai = c(0.9,0.9,0.1,0.1), omi = c(0,0,0,0))

matplot(ww, type="l", lty="solid",  las = 1,
        ylab = expression(italic(x[t])), xlab = "Time",
        col = gray(0.5, 0.2))
points(rep(0,tt), pch = "-", col = "blue", cex=1.5)
```


## Stationarity & the mean

```{r ex_ts_plot_joint_dist_3, fig.cap="Our single realization is our estimate!"}

par(mai = c(0.9,0.9,0.1,0.1), omi = c(0,0,0,0))

matplot(ww, type="l", lty="solid",  las = 1,
        ylab = expression(italic(x[t])), xlab = "Time",
        col = gray(0.5, 0.2))
lines(ww[,1], col = "blue", lwd = 2)
```


## Stationarity & the mean

If $\text{E}(x_t)$ is constant across time, we say the time series is _stationary_ in the mean


## Stationarity of time series

_Stationarity_ is a convenient assumption that allows us to describe the statistical properties of a time series.

In general, a time series is said to be stationary if there is

1. no systematic change in the mean or variance  
2. no systematic trend  
3. no periodic variations or seasonality


## Identifying stationarity

```{r, fig.align="center"}
par(mfrow = c(2,2), mai = c(0.7,0.4,0.2,0.1))

plot.ts(arima.sim(model = list(ar = 0.3, sd = 0.1), n = 100),
        las = 1, ylab = "")
mtext("A", 3, line = 0, adj = 0, xpd = NA)
plot.ts(arima.sim(model = list(ar = 0.9, sd = 0.1), n = 100),
        las = 1, ylab = "")
mtext("B", 3, line = 0, adj = 0, xpd = NA)
plot.ts(arima.sim(model = list(ar = -0.9, sd = 0.1), n = 100),
        las = 1, ylab = "")
mtext("C", 3, line = 0, adj = 0, xpd = NA)
plot.ts(arima.sim(model = list(ma = 0.1, sd = 0.1), n = 100),
        las = 1, ylab = "")
mtext("D", 3, line = 0, adj = 0, xpd = NA)
```


## Identifying stationarity

Our eyes are really bad at identifying stationarity, so we will learn some tools to help us


## Autocovariance function (ACVF)

For stationary ts, we define the _autocovariance function_ ($\gamma_k$) as

$$
\gamma_k = \text{E}([x_t - \mu][x_{t+k} - \mu])
$$

which means that

$$
\gamma_0 = \text{E}([x_t - \mu][x_{t} - \mu]) = \sigma^2
$$


## Autocovariance function (ACVF)

For stationary ts, we define the _autocovariance function_ ($\gamma_k$) as

$$
\gamma_k = \text{E}([x_t - \mu][x_{t+k} - \mu])
$$

"Smooth" time series have large ACVF for large $k$

"Choppy" time series have ACVF near 0 for small $k$


## Autocovariance function (ACVF)

For stationary ts, we define the _autocovariance function_ ($\gamma_k$) as

$$
\gamma_k = \text{E}([x_t - \mu][x_{t+k} - \mu])
$$

We can estimate $\gamma_k$ from a sample as

$$
c_k = \frac{1}{N}\sum_{t=1}^{N-k}{(x_t - m)(x_{t+k} - m)}
$$


## Autocorrelation function (ACF)

The _autocorrelation function_ (ACF) is simply the ACVF normalized by the variance

$$
\rho_k = \frac{\gamma_k}{\sigma^2} = \frac{\gamma_k}{\gamma_0}
$$

The ACF measures the correlation of a time series against a time-shifted version of itself


## Autocorrelation function (ACF)

The _autocorrelation function_ (ACF) is simply the ACVF normalized by the variance

$$
\rho_k = \frac{\gamma_k}{\sigma^2} = \frac{\gamma_k}{\gamma_0}
$$

The ACF measures the correlation of a time series against a time-shifted version of itself

We can estimate ACF from a sample as

$$
r_k = \frac{c_k}{c_0}
$$


## Properties of the ACF

The ACF has several important properties:

- $-1 \leq r_k \leq 1$  

> - $r_k = r_{-k}$  

> - $r_k$ of a periodic function is itself periodic  

> - $r_k$ for the sum of 2 independent variables is the sum of $r_k$ for each of them 


## The correlogram 

```{r, fig.cap="Graphical output for the ACF"}
par(mai=c(1,1,0,0), omi=c(0.1,0.1,0.1,0.1))
plot(NA, NA, type="n", xlim=c(0,15), ylim=c(-1,1),
     xlab="", xaxt="n", ylab="", las = 1)
abline(h=0)
axis(side=1, at=seq(15), labels=FALSE)
axis(side=1, at=seq(0,15,5))
mtext(expression(paste("Lag ", (italic(k)))), side=1, line=3, cex=1.2)
mtext(expression(paste("ACF ", (italic(r[k])))), side=2, line=3, cex=1.2)
```


## The correlogram 

```{r, fig.cap="The ACF at lag = 0 is always 1"}
par(mai=c(1,1,0,0), omi=c(0.1,0.1,0.1,0.1))
plot(NA, NA, type="n", xlim=c(0,15), ylim=c(-1,1),
     xlab="", xaxt="n", ylab="", las = 1)
abline(h=0)
axis(side=1, at=seq(15), labels=FALSE)
axis(side=1, at=seq(0,15,5))
mtext(expression(paste("Lag ", (italic(k)))), side=1, line=3, cex=1.2)
mtext(expression(paste("ACF ", (italic(r[k])))), side=2, line=3, cex=1.2)

lines(c(0,0), c(0,1), lwd=2, col="darkred")
text(x=1, y =1, expression(italic(r)[0] == 1), col="darkred")
```


## The correlogram 

```{r, fig.cap="Approximate confidence intervals"}
par(mai=c(1,1,0,0), omi=c(0.1,0.1,0.1,0.1))
plot(NA, NA, type="n", xlim=c(0,15), ylim=c(-1,1),
     xlab="", xaxt="n", ylab="", las = 1)
abline(h=0)
axis(side=1, at=seq(15), labels=FALSE)
axis(side=1, at=seq(0,15,5))
mtext(expression(paste("Lag ", (italic(k)))), side=1, line=3, cex=1.2)
mtext(expression(paste("ACF ", (italic(r[k])))), side=2, line=3, cex=1.2)

lines(c(0,0), c(0,1), lwd=2, col="darkred")
text(x=1, y =1, expression(italic(r)[0] == 1), col="darkred")

# add 95% CI's
nn <- 30
alpha <- 0.05
ts.SD <- qnorm(1-alpha/2, 0, 1)/sqrt(nn)
abline(h=-ts.SD, lty="dashed", col="blue")
text(x=14, y=-0.55, expression(-frac(italic(z)[1-frac(alpha,2)], sqrt(italic(n)))), col="blue")
abline(h=ts.SD, lty="dashed", col="blue")
text(x=14, y=0.55, expression(+frac(italic(z)[1-frac(alpha,2)], sqrt(italic(n)))), col="blue")
```


## Estimating the ACF in R

```{r acf_example, echo = TRUE , eval = FALSE}
acf(ts_object)
```


## ACF for deterministic forms

```{r}
## length of ts
nn <- 100

## trend only
par(mfrow=c(1,2), mai=c(1,1,0,0), omi=c(0.1,0.1,0.6,0.1))
tt <- seq(nn)
plot.ts(tt, ylab=expression(italic(x[t])))
acf(tt)
mtext("Linear trend {1,2,3,...,100}", outer=TRUE, line=1, cex=1.5)
```


## ACF for deterministic forms

```{r}
par(mfrow=c(1,2), mai=c(1,1,0,0), omi=c(0.1,0.1,0.6,0.1))
## compute the 2 predictor variables
tt <- sin(2*pi*seq(nn)/12)
plot.ts(tt, ylab=expression(italic(x[t])))
acf(tt)
mtext("Discrete (monthly) sine wave", outer=TRUE, line=1, cex=1.5)
```


## ACF for deterministic forms

```{r}
par(mfrow=c(1,2), mai=c(1,1,0,0), omi=c(0.1,0.1,0.6,0.1))
## compute the 2 predictor variables
tt <- sin(2*pi*seq(nn)/12) - seq(nn)/50
plot.ts(tt, ylab=expression(italic(x[t])))
acf(tt, lag.max=30)
mtext("Linear trend + seasonal effect", outer=TRUE, line=1, cex=1.5)
```


## ACF for deterministic forms

```{r}
par(mfrow=c(1,2), mai=c(1,1,0,0), omi=c(0.1,0.1,0.6,0.1))
# compute the 2 predictor variables
tt <- rep(floor(runif(nn/10,1,101)), times=10)
plot.ts(tt, ylab=expression(italic(x[t])))
acf(tt)
mtext("Sequence of 10 random numbers repeated 10 times", outer=TRUE, line=1, cex=1.5)
```


## Induced autocorrelation

Recall the transitive property, whereby

If $A = B$ and $B = C$, then $A = C$


## Induced autocorrelation

Recall the transitive property, whereby

If $A = B$ and $B = C$, then $A = C$

which suggests that

If $x \propto y$ and $y \propto z$, then $x \propto z$


## Induced autocorrelation

Recall the transitive property, whereby

If $A = B$ and $B = C$, then $A = C$

which suggests that

If $x \propto y$ and $y \propto z$, then $x \propto z$

and thus

If $x_t \propto x_{t+1}$ and $x_{t+1} \propto x_{t+2}$, then $x_t \propto x_{t+2}$


## Partial autocorrelation funcion (PACF)

The _partial autocorrelation function_ ($\phi_k$) measures the correlation between a series $x_t$ and $x_{t+k}$ with the linear dependence of $\{x_{t-1},x_{t-2},\dots,x_{t-k-1}\}$ removed


## Partial autocorrelation funcion (PACF)

The _partial autocorrelation function_ ($\phi_k$) measures the correlation between a series $x_t$ and $x_{t+k}$ with the linear dependence of $\{x_{t-1},x_{t-2},\dots,x_{t-k-1}\}$ removed


We can estimate $\phi_k$ from a sample as

$$
\phi_k =
    \begin{cases}
      \text{Cor}(x_1,x_0) = \rho_1 & \text{if } k = 1 \\
      \text{Cor}(x_k-x_k^{k-1}, x_0-x_0^{k-1}) & \text{if } k \geq 2
    \end{cases}
$$


$$
x_k^{k-1} = \beta_1 x_{k-1} + \beta_2 x_{k-2} + \dots + \beta_{k-1} x_1
$$

$$
x_0^{k-1} = \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_{k-1} x_{k-1}
$$


## Lake Washington phytoplankton

```{r lwa_phyto_ts}
library(MARSS)
data(lakeWAplankton)
lwa <- lakeWAplanktonTrans
lwa <- lwa[lwa[,"Year"] >= 1975,]
lwa <- ts(lwa, start = c(1975,1), freq = 12)
par(mai=c(1,1,0,0), omi=c(0.1,0.1,0.1,0.1))
plot.ts(lwa[,"Cryptomonas"], ylab=expression(log(italic(Cryptomonus))), las = 1)
```


## Lake Washington phytoplankton

```{r lwa_phyto_acf, fig.cap = "Autocorrelation"}
par(mai=c(1,1,0,0), omi=c(0.1,0.1,0.1,0.1))
acf(lwa[,"Cryptomonas"], na.action = na.pass, las = 1)
```


## Lake Washington phytoplankton

```{r lwa_phyto_pacf, fig.cap = "Partial autocorrelation"}
par(mai=c(1,1,0,0), omi=c(0.1,0.1,0.1,0.1))
pacf(lwa[,"Cryptomonas"], na.action = na.pass, las = 1)
```


## ACF & PACF in model selection

We will see that the ACF & PACF are _very_ useful for identifying the orders of ARMA models



## Cross-covariance function (CCVF)

Often we want to look for relationships between 2 different time series

We can extend the notion of covariance to _cross-covariance_


## Cross-covariance function (CCVF)

Often we want to look for relationships between 2 different time series

We can extend the notion of covariance to _cross-covariance_

We can estimate the CCVF $(g^{x,y}_k)$ from a sample as

$$
g^{x,y}_k = \frac{1}{N}\sum_{t=1}^{N-k}{(x_t - m_x)(y_{t+k} - m_y)}
$$


## Cross-correlation function (CCF)

The cross-correlation function is the CCVF normalized by the standard deviations of x & y

$$
r^{x,y}_k = \frac{g^{x,y}_k}{s_x s_y}
$$

Just as with other measures of correlation

$$
-1 \leq r^{x,y}_k \leq 1
$$


## Estimating the CCF in R

```{r ccf_example, echo = TRUE , eval = FALSE}
ccf(x, y)
```

**Note**: the lag `k` value returned by `ccf(x, y)` is the correlation between `x[t+k]` and `y[t]`

In an explanatory context, we often think of $y = f(x)$, so it's helpful to use `ccf(y, x)` and only consider positive lags


## Example of cross-correlation

```{r, fig.align="center"}
## get the matching years of sunspot data
suns <- ts.intersect(lynx,sunspot.year)[,"sunspot.year"]
## get the matching lynx data
lynx <- ts.intersect(lynx,sunspot.year)[,"lynx"]

layout(mat = matrix(c(1,1,2,2,0,3,3,0), 4, 2))

par(mai=c(0.1,0.6,0.1,0.3), omi=c(0.5,0,0,0))
plot.ts(suns, main="", ylab="Sunspot activity",
        xaxt="n", xlab="", cex.lab = 2)

# par(mai=c(0.6,0.5,0.1,0.1), omi=c(0,0,0,0))
plot.ts(lynx, ylab="Number of trapped lynx")

# par(mai=c(0.6,0.5,0,0.1))
ccf(log(lynx), log(suns), ylab="Cross-correlation", main="")
```



