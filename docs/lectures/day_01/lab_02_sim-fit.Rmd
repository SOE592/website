---
title: "Simulating and fitting time series"
subtitle: "SOE 592 â€“ Intro to Time Series Analysis"
date: "<br>3 Jan 2024"
output:
  html_document:
    theme: simplex
    highlight: textmate
    css: ["lecture_inst.css", "fontawesome.css", "solid.css"]
    toc: true
    toc_float: true
    toc_depth: 3
---

```{r setup, echo = FALSE}
knitr::opts_chunk$set(
  fig.dim = c(7, 5),
  fig.align = "center"
  )
```

***

# Introduction

One of the best ways to learn about how models work is to simulate data and then try to recover the parameters of interest. In this lab we'll simulate data from some of the simple time series models we discuss earlier, and then use some other functions to plot them and investigate their properties.

***

# White noise

Recall that a time series $\{w_t\}$ is discrete white noise if its values are

1. independent  

2. identically distributed with a mean of zero

In this course we will focus exclusively on _Gaussian white noise_, whereby

$$
w_t \sim \text{N}(0,\sigma^2)
$$

## Simulating white noise

Drawing random numbers from a statistical distribution in R is straightforward. For the normal (Gaussian) distribution, the function is `rnorm()`. Note that we will typically refer to the variance of a distribution rather than the standard deviation (SD), so be careful to use the correct value when specifying the form of the distribution.

<div class="boxy boxy-orange boxy-lightbulb">
**Tip:** Type `?rnorm` at the command prompt to see all of the options for `rnorm()`.
</div>


```{r sim_white_noise}
## set the seed for the random number generator so we all get the same results
set.seed(592)

## length of the time series
nn <- 100

## set the variance = 2 --> SD = sqrt(2)
sigma <- sqrt(2)

## draw random values
ww <- rnorm(n = nn, mean = 0, sd = sigma)
```

## Plotting white noise

We can use the base function `plot.ts()` to plot our white noise sequence.

```{r plot_white_noise}
## plot the time series of white noise
plot.ts(x = ww, ylab = expression(italic(w[t])))
```

<div class="boxy boxy-orange boxy-lightbulb">
**Tip:** You can use pipe operators with `plot.ts()` just as you would for any other function, independent of whether it comes from the {tidyverse} package.
</div>

```{r plot_white_noise_pipe, eval = FALSE}
## plot.ts via the native pipe operator |>
ww |> plot.ts(ylab = expression(italic(w[t])))

## plot.ts via the {tidyverse} pipe operator %>%
ww %>% plot.ts(ylab = expression(italic(w[t])))
```

***

# Autocorrelation function

Recall from lecture that we can estimate the correlation of a time series with a lagged (shifted) version of itself via the _autocorrelation function_ (ACF). If a time series is white noise, the ACF should equal zero for all non-zero lags.

We can estimate the ACF with the base function `acf()`, which will consider lags up to 20 by default.

<div class="boxy boxy-orange boxy-lightbulb">
**Tip:** Type `?acf` at the command prompt to see all of the options for `acf()`, including how to set the maximum number of lags via the `lag.max` argument.
</div>

```{r acf_white_noise}
## estimate and plot the ACF
acf(x = ww)
```

<div class="boxy boxy-red boxy-exclamation">
**Note:** The autocorrelation at a lag of 0 equals 1 because it's the correlation of a time series with itself.
</div>

<br>

**Q**: What's going on at lag = 5 where it looks like the correlation is below the dashed blue line indicating it's significantly negative?

**A**: Here we're basing the statistical significance on a Type-I error of 5% (i.e., $\alpha$ = 0.05), which means we should expect a spurious significant correlation about 1 in 20 times. In this case, we were simply (un)lucky.

<br>

<div class="boxy boxy-blue boxy-clipboard-list">
**Task:** Increase the length of the simulated time series with `n = 1000` and estimate the ACF. Do you see significant autocorrelation at any lags?
</div>


***

# Random walks

Random walks are one of the most common and most simple time series models. They are amazingly flexible in their realizations and can be fit to many different "shapes" of data. Recall that a time series $\{x_t\}$ is a random walk if

1. $x_t = x_{t-1} + w_t$  

2. $w_t$ is white noise

In this course we'll focus exclusively on Gaussian white noise, which means our random walk model is given by

$$
x_t = x_{t-1} + w_t \\
\\
w_t \sim \text{N}(0,\sigma^2)
$$

## Simulating random walks

Simulating random walks is rather straightforward. To begin, we'll use the same white noise sequence we simulated above as our process errors. We just need to specify a starting value for $x_1$ and then iteratively add one error term each time step. This is easy to do with a `for()` loop in R.

Let's begin by implicitly setting $x_0 = 0$, which means that $x_1 = w_1$.

```{r sim_RW}
## initialize a vector for storing x_t by setting it equal to w_t
xx <- ww

## specify the initial value at t = 1
xx[1] <- ww[1]

## loop over time steps 2 through 100
for(t in 2:nn) {
  xx[t] <- xx[t-1] + ww[t]
}
```

## Plotting our random walk

Let's again use the base function `plot.ts()` to plot the data from the random walk model we just created.

```{r plot_RW}
## plot the realization of our random walk model
plot.ts(x = xx, ylab = expression(italic(x[t])))
```

<div class="boxy boxy-blue boxy-clipboard-list">
**Task:** Compare this plot to the one above with the white noise sequence. What are some of the differences you see?
</div>

<div class="boxy boxy-blue boxy-clipboard-list">
**Task:** Create a new time series of white noise and use it to create a new realization of a random walk by modifying the code above. Compare it to the plot above. What do you notice?
</div>

<br>

## ACF for a random walk

Let's examine the ACF for the vector of values from the first random walk we [simulated above](#simulating-random-walks).

```{r acf_RW}
## estimate and plot the ACF
acf(x = xx)
```

<div class="boxy boxy-blue boxy-clipboard-list">
**Task:** Compare the ACF for the random walk to that for white noise. What do you notice?
</div>

<br>

## Alternate method for RW's

Let's look at another method for simulating random walks. We'll begin by assuming that the first value $x_0$ is equal to 0 as we did above. At $t = 1$ we have

$$
x_1 = x_0 + w_1 \\
\Downarrow \\
x_1 = w_1
$$

At the next time step when $t = 2$, we then have

$$
x_2 = x_1 + w_2 \\
\Downarrow \\
x_2 = w_1 + w_2
$$

At time step $t = 3$ we have

$$
x_3 = x_2 + w_3 \\
\Downarrow \\
x_3 = (w_1 + w_2) + w_3
$$

When $t = 4$ we next have

$$
x_4 = x_3 + w_4 \\
\Downarrow \\
x_4 = (w_1 + w_2 + w_3) + w_4
$$

and so on. This implies that a random walk is simply the cumulative sum of a white noise sequence, which we can accomplish with the `cumsum()` function. We can then compare that version to our original calculation with the correlation function `cor()` (i.e., the correlation of a vector with itself is 1).

```{r cumsum_RW}
## cumulative sum of our white noise sequence
xx2 <- cumsum(ww)

## correlation between xx & xx2
cor(xx, xx2)
```

<div class="boxy boxy-success boxy-check">
**Success:** Wow--it worked!
</div>


***

# Biased random walks

Biased random walks are also common models in ecology. For example, as we will see later in the course, you can write a simple model for exponential population growth in discrete time as a biased random walk. Recall that a _biased random walk_ (or _random walk with drift_) is written as

$$
x_t = x_{t-1} + u + w_t \\
\\
w_t \sim \text{N}(0,\sigma^2)
$$  

where $u$ is the bias (drift) per time step and $w_t$ is white noise.


## Simulating a biased random walk

Simulating a biased random walk is just as easy as a regular random walk. Here we'll use the same white noise sequence we simulated above as our process errors. We just need to specify a starting value for $x_1$ and then iteratively add the bias and one error term each time step. This is easy to do with a `for()` loop in R.

Let's begin by implicitly setting $x_0 = 0$, which means that $x_1 = u + w_1$.

```{r ex_biased_RW}
## initialize a vector to store the values
xb <- ww

## set the bias equal to 1
uu <- 1

## set the initial value
xb[1] <- uu + ww[1]

for(t in 2:nn) {
  xb[t] <- xb[t-1] + uu + ww[t]
}
```

## Plotting a biased random walk

We'll again use the base function `plot.ts()` to plot the data from the biased random walk model we just created.

```{r plot_biased_RW}
## plot the realization of our random walk model
plot.ts(x = xb, ylab = expression(italic(x[t])))
```

<div class="boxy boxy-blue boxy-clipboard-list">
**Task:** Change the bias term to be something much smaller (e.g., 0.1), re-run the code above and then plot the new series. How does it compare to the first example?
</div>

<br>

## ACF for a biased random walk

Let's examine the ACF for the vector of values from the biased random walk we just simulated. As before, we can use the `acf()` function.

```{r acf_biased_RW}
## estimate and plot the ACF
acf(x = xb)
```

<div class="boxy boxy-blue boxy-clipboard-list">
**Task:** Compare the ACF for this biased random walk to that for the simple random walk above. What do you notice?
</div>

<br>

***

# Autoregressive (AR) models

Recall that an autoregressive model of order _p_, or AR(_p_), is defined as

$$
x_t = \phi_1 x_{t-1} + \phi_2 x_{t-2} + \dots + \phi_p x_{t-p} + w_t
$$

where we assume

1. $w_t$ is white noise

2. $\phi_p \neq 0$ for an order-_p_ process

The random walk and biased random walk models we saw above are two of the most common autoregressive models, both of which are AR(1) with $\phi$ = 1.

Here we will look at some other forms of AR(_p_) models where $|\phi_i| < 1$, which are _stationary_ in the mean and variance.

## Simulating an AR(_p_) model

Simulating an AR(_p_) model is also straightforward. One option is to use a `for()` loop like we did above for the [random walk](#simulating-random-walks) and [biased random walk](#simulating-a-biased-random-walk) models. The AR in ARIMA stands for "autoregressive", the I for "integrated", and the MA for "moving-average"; we specify the order of ARIMA models as $p,d,q$.  So, for example, we would specify an AR(2) model as ARIMA(2,0,0), or an MA(1) model as ARIMA(0,0,1).  If we had an ARMA(3,1) model that we applied to data that had been twice-differenced, then we would have an ARIMA(3,2,1) model. 

Here we'll explore how to use the `arima.sim()` function in base R.

<div class="boxy boxy-orange boxy-lightbulb">
**Tip:** Type `?arima.sim` at the command prompt to see all of the options for `arima.sim()`.
</div>

`arima.sim()` will accept many arguments, but we are interested primarily in three of them:

1. `n`: the length of desired time series 

2. `model`: a list with the following elements

    i) `order`: a vector of length 3 containing the ARIMA($p,d,q$) order
    
    ii) `ar`: a vector of length $p$ containing the AR(_p_) coefficients
    
    iii) `ma`: a vector of length $q$ containing the MA(_q_) coefficients

3. `sd`: the standard deviation of the Gaussian errors

<div class="boxy boxy-orange boxy-lightbulb">
**Tip:** You can omit the `ma` element entirely if you have an AR(_p_) model, or omit the `ar` element if you have an MA(_q_) model.  If you omit the `sd` element, `arima.sim()` will assume you want normally distributed errors with SD = 1.  Also note that you can pass `arima.sim()` your own time series of random errors or the name of a function that will generate the errors (*e.g.*, you could use `rpois()` if you wanted a model with Poisson errors).
</div>

Let's begin by simulating some AR(1) models and comparing their behavior.  First, let's choose models with contrasting AR coefficients.  Recall that in order for an AR(1) model to be stationary, $\phi < \lvert 1 \rvert$, so we'll try 0.1 and 0.9.  We'll again set the random number seed so we will get the same answers.

<div class="boxy boxy-blue boxy-clipboard-list">
**Task:** Simulate two AR(1) models with contrasting coefficients.
</div>

```{r simAR1, echo = TRUE, eval = TRUE}
## list description for AR(1) model with a small coef
AR_sm <- list(order = c(1, 0, 0), ar = 0.1)

## list description for AR(1) model with a large coef
AR_lg <- list(order = c(1, 0, 0), ar = 0.9)

## simulate AR(1)
AR1_sm <- arima.sim(n = 50, model = AR_sm, sd = 0.1)
AR1_lg <- arima.sim(n = 50, model = AR_lg, sd = 0.1)
```

<div class="boxy boxy-blue boxy-clipboard-list">
**Task:** Plot the simulated series.
</div>

```{r plotAR1sims, eval = FALSE, echo = TRUE}
## setup plot region
par(mfrow = c(1, 2))

## get y-limits for common plots
ylm <- c(min(AR1_sm, AR1_lg), max(AR1_sm, AR1_lg))

## plot the ts
plot.ts(AR1_sm,
  ylim = ylm,
  ylab = expression(italic(x)[italic(t)]),
  main = expression(paste(phi, " = 0.1"))
)
plot.ts(AR1_lg,
  ylim = ylm,
  ylab = expression(italic(x)[italic(t)]),
  main = expression(paste(phi, " = 0.9"))
)
```

```{r plotAR1contrast, eval = TRUE, echo = FALSE, fig.dim = c(8, 4.5), fig.cap = "Time series of simulated AR(1) processes with a small (left) and large (right) cofficient."}
## get y-limits for common plots
ylm <- c(min(AR1_sm, AR1_lg), max(AR1_sm, AR1_lg))

## set the margins & text size
par(mfrow = c(1, 2), mai = c(1.2, 1, 0.3, 0), omi = c(0, 0, 0.5, 1), cex = 1)

## plot the ts
plot.ts(AR1_sm,
  ylim = ylm,
  ylab = expression(italic(x)[italic(t)]),
  main = expression(paste(phi, " = 0.1"))
)
plot.ts(AR1_lg,
  ylim = ylm,
  ylab = expression(italic(x)[italic(t)]),
  main = expression(paste(phi, " = 0.9"))
)
```
 
<br>

<div class="boxy boxy-blue boxy-clipboard-list">
**Task:** What do you notice about these two plots?
</div>

It looks like the time series with the smaller AR coefficient is more "choppy" and seems to stay closer to 0 whereas the time series with the larger AR coefficient appears to wander around more.  Remember that as the coefficient in an AR(1) model goes to 0, the model approaches a WN sequence, which is stationary in both the mean and variance.  As the coefficient goes to 1, however, the model approaches a random walk, which is not stationary in either the mean or variance.

<div class="boxy boxy-blue boxy-clipboard-list">
**Task:** Generate two AR(1) models that have the same magnitude of coeficient, but opposite signs, and compare their behavior.
</div>

```{r simAR1opps, echo = TRUE, eval = TRUE}
## list description for AR(1) model with positive coef
AR_pos <- list(order = c(1, 0, 0), ar = 0.5)

## list description for AR(1) model with negative coef
AR_neg <- list(order = c(1, 0, 0), ar = -0.5)

## simulate AR(1)
AR1_pos <- arima.sim(n = 50, model = AR_pos, sd = 0.1)
AR1_neg <- arima.sim(n = 50, model = AR_neg, sd = 0.1)
```

<div class="boxy boxy-blue boxy-clipboard-list">
**Task:** Plot the simulated series.
</div>

```{r plotAR1oppsEcho, eval = FALSE, echo = TRUE}
## setup plot region
par(mfrow = c(1, 2))

## get y-limits for common plots
ylm <- c(min(AR1_pos, AR1_neg), max(AR1_pos, AR1_neg))

## plot the ts
plot.ts(AR1_pos,
  ylim = ylm,
  ylab = expression(italic(x)[italic(t)]),
  main = expression(paste(phi[1], " = 0.5"))
)
plot.ts(AR1_neg,
  ylab = expression(italic(x)[italic(t)]),
  main = expression(paste(phi[1], " = -0.5"))
)
```

```{r plotAR1opps, eval = TRUE, echo = FALSE, fig.dim = c(8, 4.5), fig.cap = "Time series of simulated AR(1) processes with a positive (left) and negative (right) cofficient of the same magnitude."}
## get y-limits for common plots
ylm <- c(min(AR1_pos, AR1_neg), max(AR1_pos, AR1_neg))

## set the margins & text size
par(mfrow = c(1, 2), mai = c(1.2, 1, 0.3, 0), omi = c(0, 0, 0.5, 1), cex = 1)

## plot the ts
plot.ts(AR1_pos,
  ylim = ylm,
  ylab = expression(italic(x)[italic(t)]),
  main = expression(paste(phi[1], " = 0.5"))
)
plot.ts(AR1_neg,
  ylim = ylm,
  ylab = expression(italic(x)[italic(t)]),
  main = expression(paste(phi[1], " = -0.5"))
)
```
 
<br>

<div class="boxy boxy-blue boxy-clipboard-list">
**Task:** What do you notice about these two plots?
</div>

It appears like both time series vary around the mean by about the same amount, but the model with the negative coefficient produces a much more "sawtooth" time series.  It turns out that _any_ AR(1) model with $-1<\phi<0$ will exhibit the 2-point oscillation you see here.

<div class="boxy boxy-orange boxy-lightbulb">
**Tip:** We can simulate higher order AR(_p_) models in the same manner, but care must be exercised when choosing a set of coefficients that result in a stationary model or else `arima.sim()` will fail and report an error.
</div>

For example, an AR(2) model with both coefficients equal to 0.5 is not stationary, and therefore this function call will not work:

```{r AR_p_coefFail, eval = FALSE, echo = TRUE}
arima.sim(n = 100, model = list(order(2, 0, 0), ar = c(0.5, 0.5)))
```

If you try, R will respond with an error that the `'ar' part of model is not stationary`.


## Correlation of AR(_p_) processes

Let's review what we learned in lecture about the general behavior of the ACF and PACF for AR(_p_) models.  To do so, we'll simulate four stationary AR(_p_) models of increasing order $p$ and then examine their ACF's and PACF's.  Let's use a really big $n$ so as to make them "pure", which will provide a much better estimate of the correlation structure.

<div class="boxy boxy-blue boxy-clipboard-list">
**Task:** Simulate four stationary AR(_p_) models of increasing order $p$.
</div>

```{r AR_p_coefSims, eval = TRUE, echo = TRUE}
## the 4 AR coefficients
AR_p_coef <- c(0.7, 0.2, -0.1, -0.3)

## empty list for storing models
AR_mods <- list()

## loop over orders of p
for (p in 1:4) {
  ## assume sd = 1, so not specified
  AR_mods[[p]] <- arima.sim(n = 10000, list(ar = AR_p_coef[1:p]))
}
```

<div class="boxy boxy-blue boxy-clipboard-list">
**Task:** Now that we have our four AR(_p_) models, lets look at plots of the time series, ACF's, and PACF's.
</div>

```{r plotAR_p_coefCompsEcho, eval = FALSE, echo = TRUE}
## set up plot region
par(mfrow = c(4, 3))

## loop over orders of p
for (p in 1:4) {
  plot.ts(AR_mods[[p]][1:50],
    ylab = paste("AR(", p, ")", sep = "")
  )
  acf(AR_mods[[p]], lag.max = 12)
  pacf(AR_mods[[p]], lag.max = 12, ylab = "PACF")
}
```

```{r plotAR-p-coefComps, eval = TRUE, echo = FALSE, fig.dim = c(8, 8), fig.cap = "Time series of simulated AR(_p_) processes (left column) of increasing orders from 1-4 (rows) with their associated ACF's (center column) and PACF's (right column).  Note that only the first 50 values of $x_t$ are plotted."}
## set the margins & text size
par(mfrow = c(4, 3), mar = c(4, 4, 0.5, 0.5), omi = c(0.3, 0, 0, 0), cex = 1)

## loop over orders of p
for (p in 1:4) {
  plot.ts(AR_mods[[p]][1:50], ylab = paste("AR(", p, ")", sep = ""))
  acf(AR_mods[[p]], lag.max = 12)
  pacf(AR_mods[[p]], lag.max = 12, ylab = "PACF")
}
```
 
<br>

<div class="boxy boxy-orange boxy-lightbulb">
**Tip:** As we saw in lecture and is evident from our examples shown above, the ACF for an AR(_p_) process tails off toward zero very slowly, but the PACF goes to zero for lags > $p$.  This is an important diagnostic tool when trying to identify the order of $p$ in ARMA(_p_,_q_) models.
</div>

***

# Moving-average (MA) models

A moving-averge process of order $q$, or MA(_q_), is a weighted sum of the current random error plus the $q$ most recent errors, and can be written as

\begin{equation}
x_t = w_t + \theta_1 w_{t-1} + \theta_2 w_{t-2} + \dots + \theta_q w_{t-q},
\end{equation}

where $\{w_t\}$ is a white noise sequence with zero mean and some variance $\sigma^2$; for our purposes we usually assume that $w_t \sim \text{N}(0,q)$.  Of particular note is that because MA processes are finite sums of stationary errors, they themselves are stationary.

## Simulating an MA(_q_) process

We can simulate MA(_q_) processes just as we did for AR(_p_) processes using `arima.sim()`.  Here are 3 different ones with contrasting $\theta$'s.

```{r simMA1opps, echo = TRUE, eval = TRUE}
## list description for MA(1) model with small coef
MA_sm <- list(order = c(0, 0, 1), ma = 0.2)

## list description for MA(1) model with large coef
MA_lg <- list(order = c(0, 0, 1), ma = 0.8)

## list description for MA(1) model with large coef
MA_neg <- list(order = c(0, 0, 1), ma = -0.5)

## simulate MA(1)
MA1_sm <- arima.sim(n = 50, model = MA_sm, sd = 0.1)
MA1_lg <- arima.sim(n = 50, model = MA_lg, sd = 0.1)
MA1_neg <- arima.sim(n = 50, model = MA_neg, sd = 0.1)
```

And here are their associated plots.

```{r plotMA1oppsEcho, eval = FALSE, echo = TRUE}
## setup plot region
par(mfrow = c(1, 3))

## plot the ts
plot.ts(MA1_sm,
  ylab = expression(italic(x)[italic(t)]),
  main = expression(paste(theta, " = 0.2"))
)
plot.ts(MA1_lg,
  ylab = expression(italic(x)[italic(t)]),
  main = expression(paste(theta, " = 0.8"))
)
plot.ts(MA1_neg,
  ylab = expression(italic(x)[italic(t)]),
  main = expression(paste(theta, " = -0.5"))
)
```

```{r plotMA1opps, eval = TRUE, echo = FALSE, fig.dim = c(8,4), fig.cap = "Time series of simulated MA(1) processes with varying signs and magnitudes of coefficients."}
## set the margins & text size
par(mfrow = c(1, 3), mar = c(4, 4, 1.5, 0.5), omi = c(0.3, 0, 0, 0), cex = 1)

## plot the ts
plot.ts(MA1_sm,
  ylab = expression(italic(x)[italic(t)]),
  main = expression(paste(theta, " = 0.2"))
)
plot.ts(MA1_lg,
  ylab = expression(italic(x)[italic(t)]),
  main = expression(paste(theta, " = 0.8"))
)
plot.ts(MA1_neg,
  ylab = expression(italic(x)[italic(t)]),
  main = expression(paste(theta, " = -0.5"))
)
```
 
<br>

<div class="boxy boxy-red boxy-exclamation">
**Note:** In contrast to AR(1) processes, MA(1) models do not exhibit radically different behavior with changing $\theta$.  This should not be too surprising given that they are simply linear combinations of white noise.
</div>


## Correlation of MA(_q_) processes

We saw in lecture and above how the ACF and PACF have distinctive features for AR(_p_) models, and they do for MA(_q_) models as well.  Here are examples of four MA(_q_) processes.  As before, we'll use a really big $n$ so as to make them "pure", which will provide a much better estimate of the correlation structure.

```{r MA_q_coefSims, eval = TRUE, echo = TRUE}
## the 4 MA coefficients
MA_q_coef <- c(0.7, 0.2, -0.1, -0.3)

## empty list for storing models
MA_mods <- list()

## loop over orders of q
for (q in 1:4) {
  ## assume sd = 1, so not specified
  MA_mods[[q]] <- arima.sim(n = 1000, list(ma = MA_q_coef[1:q]))
}
```

Now that we have our four MA(_q_) models, lets look at plots of the time series, ACF's, and PACF's.

```{r plotMApCompsEcho, eval = FALSE, echo = TRUE}
## set up plot region
par(mfrow = c(4, 3))

## loop over orders of q
for (q in 1:4) {
  plot.ts(MA_mods[[q]][1:50],
    ylab = paste("MA(", q, ")", sep = "")
  )
  acf(MA_mods[[q]], lag.max = 12)
  pacf(MA_mods[[q]], lag.max = 12, ylab = "PACF")
}
```

```{r plotMApComps, eval = TRUE, echo = FALSE, fig.dim = c(8, 8), fig.cap = "Time series of simulated MA(_q_) processes (left column) of increasing orders from 1-4 (rows) with their associated ACF's (center column) and PACF's (right column).  Note that only the first 50 values of $x_t$ are plotted."}
## set the margins & text size
par(mfrow = c(4, 3), mar = c(4, 4, 0.5, 0.5), omi = c(0.3, 0, 0, 0), cex = 1)

## loop over orders of q
for (q in 1:4) {
  plot.ts(MA_mods[[q]][1:50], ylab = paste("MA(", q, ")", sep = ""))
  acf(MA_mods[[q]], lag.max = 12)
  pacf(MA_mods[[q]], lag.max = 12, ylab = "PACF")
}
```
 
<br>

<div class="boxy boxy-orange boxy-lightbulb">
**Tip:** As we saw in lecture and is evident from our examples here, the ACF for an MA(_q_) process goes to zero for lags > $q$, but the PACF tails off toward zero very slowly.  This is an important diagnostic tool when trying to identify the order of $q$ in ARMA(_p_,_q_) models.
</div>

***

# Fitting ARMA(_p_,_q_) models

We have already seen how to simulate AR(_p_) and MA(_q_) models with `arima.sim()`; the same concepts apply to ARMA(_p_,_q_) models and therefore we will not do that here.  Instead, we will move on to fitting ARMA(_p_,_q_) models when we only have a realization of the process (*i.e.*, the data) and do not know the underlying parameters that generated it.

The function `arima()` accepts a number of arguments, but two of them are most important:

* `x`: a univariate time series
* `order`: a vector of length 3 specifying the order of ARIMA(p,d,q) model

<div class="boxy boxy-orange boxy-lightbulb">
**Tip:** Type `?arima` at the command prompt to see all of the options.
</div>

<div class="boxy boxy-red boxy-exclamation">
**Note:** By default, `arima()` will estimate an underlying mean of the time series unless $d>0$.
</div>

For example, an AR(1) process with nonzero mean $\mu$ would be written

\begin{equation}
x_t = \mu + \phi (x_{t-1} - \mu) + w_t.
\end{equation}

<div class="boxy boxy-red boxy-exclamation">
**Note:** If you know for a fact that the time series has a mean of zero (*e.g.*, you already subtracted the mean), you should include the argument `include.mean = FALSE`, which is set to `TRUE` by default.  Ignoring and not estimating a mean in ARMA(_p_,_q_) models when one exists will bias the estimates of all other parameters.
</div>

Let's see an example of how `arima()` works.  First we'll simulate an ARMA(2,2) model and then estimate the parameters to see how well we can recover them.  In addition, we'll add in a constant to create a non-zero mean, which `arima()` reports as `intercept` in its output.

<div class="boxy boxy-blue boxy-clipboard-list">
**Task:** Simulate data from an ARMA(2,2) model and estimate the parameters.
</div>

```{r ARMAest, eval = TRUE, echo = TRUE}
## ARMA(2,2) description for arim.sim()
ARMA22 <- list(order = c(2, 0, 2), ar = c(-0.7, 0.2), ma = c(0.7, 0.2))

## mean of process
mu <- 5

## simulated process (+ mean)
ARMA_sim <- arima.sim(n = 10000, model = ARMA22) + mu

## estimate parameters
arima(x = ARMA_sim, order = c(2, 0, 2))
```

It looks like we were pretty good at estimating the true parameters, but our sample size was admittedly quite large; the estimate of the variance of the process errors is reported as `sigma^2` below the other coefficients.  

<div class="boxy boxy-blue boxy-clipboard-list">
**Task:** Try decreasing the length of time series in the  `arima.sim()` call above from 10,000 to something like 100 and see what effect it has on the parameter estimates.
</div>


## Searching over model orders

In an ideal situation, you could examine the ACF and PACF of the time series of interest and immediately decipher what orders of $p$ and $q$ must have generated the data, but that doesn't always work in practice.  Instead, we are often left with the task of searching over several possible model forms and seeing which of them provides the most parsimonious fit to the data.  There are two easy ways to do this for ARIMA models in R.

The first is to write a little script that loops ove the possible dimensions of $p$ and $q$.  Let's try that for the process we simulated above and search over orders of $p$ and $q$ from 0-3 (it will take a few moments to run and will likely report an error about a "`possible convergence problem`", which you can ignore).

<div class="boxy boxy-blue boxy-clipboard-list">
**Task:** Use a `for()` loop to search over various orders of ARMA(_p,_,_q_) models.
</div>

```{r ARMAsearch1, eval = TRUE, echo = TRUE, cache=TRUE}
## empty list to store model fits
ARMA_res <- list()

## set counter for model index
cc <- 1

## loop over AR
for (p in 0:3) {
  ## loop over MA
  for (q in 0:3) {
    ARMA_res[[cc]] <- arima(x = ARMA_sim, order = c(p, 0, q))
    cc <- cc + 1
  }
}

## get AIC values for model evaluation
ARMA_AIC <- sapply(ARMA_res, function(x) x$aic)

## model with lowest AIC is the best
ARMA_res[[which(ARMA_AIC == min(ARMA_AIC))]]
```

<div class="boxy boxy-success boxy-check">
**Success:** It looks like our search worked, so let's look at the other method for fitting ARIMA models.
</div>

The `auto.arima()` function in the {forecast} package will conduct an automatic search over all possible orders of ARIMA models that you specify.  For details, type `?auto.arima` after loading the package.

<div class="boxy boxy-blue boxy-clipboard-list">
**Task:** Repeat the model search process using the same criteria as above.
</div>

```{r autoARIMA, eval = TRUE, echo = TRUE, cache = TRUE}
## find best ARMA(p,q) model
forecast::auto.arima(ARMA_sim, start.p = 0, max.p = 3, start.q = 0, max.q = 3)
```

<div class="boxy boxy-success boxy-check">
**Success:** We get the same results with an increase in speed and less coding, which is nice.
</div>

<div class="boxy boxy-orange boxy-lightbulb">
**Tip:** If you want to see the form for each of the models checked by `auto.arima()` and their associated AIC values, include the argument `trace = 1`.
</div>

